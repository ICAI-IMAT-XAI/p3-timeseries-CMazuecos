{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4bc946c",
   "metadata": {},
   "source": [
    "# Lab: XAI and Uncertainty for Time-Series Forecasting (Panama Load)\n",
    "\n",
    "You are data scientists at a grid operator in Panama. Your task is short-term load forecasting, interpreting model behaviour (XAI), and estimating predictive uncertainty so operators can make safer operational decisions. This notebook walks you through EDA, feature engineering, training a model, SHAP explanations, temporal attribution with occlusion, bootstrap-based variability, and conformal prediction intervals.\n",
    "\n",
    "Follow the TODO markers in code cells. The dataset is expected at `data/panama_load.csv` and should contain at least a timestamp column and a load column. Optional covariates (temperature, humidity, etc.) are welcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f50643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 0 - Setup\n",
    "# Imports, plotting style, and random seed\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plot style and seed\n",
    "sns.set_style('whitegrid')\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e647c",
   "metadata": {},
   "source": [
    "## Section 1 — Load data and basic EDA\n",
    "\n",
    "Goal: understand the time series, inspect seasonality, and spot anomalies. Follow instructions and fill TODOs.\n",
    "\n",
    "You can learn more about the dataset at: https://www.kaggle.com/datasets/saurabhshahane/electricity-load-forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load data into a DataFrame\n",
    "path = '../data/panama_load.csv'\n",
    "if not os.path.exists(path):\n",
    "    print(f'WARNING: {path} not found. Please place the dataset at this path.')\n",
    "df = pd.read_csv(path)\n",
    "print(f'Data loaded with shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dee2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_col = 'datetime' \n",
    "load_col = 'nat_demand'  \n",
    "# Attempt to parse a datetime column robustly\n",
    "if datetime_col not in df.columns:\n",
    "    # try to guess a datetime-like column\n",
    "    for c in df.columns:\n",
    "        if 'date' in c.lower() or 'time' in c.lower():\n",
    "            datetime_col = c\n",
    "            break\n",
    "print('Using datetime column:', datetime_col)\n",
    "# Parse datetimes\n",
    "try:\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col])\n",
    "except Exception as e:\n",
    "    print('Error parsing datetimes:', e)\n",
    "# Set index and sort\n",
    "df = df.set_index(datetime_col).sort_index()\n",
    "\n",
    "print('Using load column:', load_col)\n",
    "# Keep a clean dataframe with at least datetime index and load\n",
    "data = df[[load_col]].copy()\n",
    "data.columns = ['load']\n",
    "data = data.sort_index()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7569f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Basic plots: entire series, one month, a couple of days\n",
    "fig, ax = plt.subplots(3, 1, figsize=(14, 10), sharex=False)\n",
    "# Entire series\n",
    "data['load'].plot(ax=ax[0], title='Entire time series (load)')\n",
    "# Zoom into one month (choose the last available month)\n",
    "last = data.index.max()\n",
    "month_start = (last - pd.Timedelta(days=30)).ceil('D')\n",
    "data.loc[month_start:last, 'load'].plot(ax=ax[1], title='Zoom: last 30 days')\n",
    "# Zoom into a couple of days\n",
    "days_start = (last - pd.Timedelta(days=3)).ceil('D')\n",
    "data.loc[days_start:last, 'load'].plot(ax=ax[2], title='Zoom: last 3 days')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8129c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Average load by hour-of-day and day-of-week\n",
    "df_feats = data.copy()\n",
    "df_feats['hour'] = df_feats.index.hour\n",
    "df_feats['dow'] = df_feats.index.dayofweek\n",
    "hourly = df_feats.groupby('hour')['load'].mean()\n",
    "dow = df_feats.groupby('dow')['load'].mean()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "hourly.plot(kind='bar', ax=ax1, title='Average load by hour of day')\n",
    "ax1.set_xlabel('Hour of day')\n",
    "ax1.set_ylabel('Load')\n",
    "dow.plot(kind='bar', ax=ax2, title='Average load by day of week')\n",
    "ax2.set_xlabel('Day of week (0=Mon)')\n",
    "ax2.set_ylabel('Load')\n",
    "plt.tight_layout()\n",
    "plot_acf(data['load'].dropna(), lags=60)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f005ff4",
   "metadata": {},
   "source": [
    "- TODO: Inspect the previous plots and comment on the patterns you can distinguish\n",
    "\n",
    ">>> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5185625",
   "metadata": {},
   "source": [
    "## Section 2 — Turn time series into supervised learning data\n",
    "\n",
    "We convert the time series into a tabular supervised problem: predict 1-step-ahead load from lag features, rolling statistics and calendar features.\n",
    "Fill TODOs in the helper function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f13ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_supervised(df, target_col='load', horizon=1, lags=None):\n",
    "    \"\"\"Create supervised features for a 1-step-ahead forecast.\"\"\"\n",
    "    if lags is None:\n",
    "        lags = [1, 24, 48, 168]  # t-1, t-24 (daily), t-48, t-168 (weekly)\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    # Lag features\n",
    "    for lag in lags:\n",
    "        X[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    # Rolling features: 24-hour and 7-day rolling mean/std\n",
    "    X['rmean_24'] = df[target_col].rolling(window=24, min_periods=1).mean().shift(1)\n",
    "    X['rstd_24'] = df[target_col].rolling(window=24, min_periods=1).std().shift(1)\n",
    "    X['rmean_168'] = df[target_col].rolling(window=168, min_periods=1).mean().shift(1)\n",
    "    # Calendar features\n",
    "    X['hour'] = df.index.hour\n",
    "    X['dow'] = df.index.dayofweek\n",
    "    X['is_weekend'] = (X['dow'] >= 5).astype(int)\n",
    "    X['month'] = df.index.month\n",
    "    # Target (1-step ahead by default)\n",
    "    y = df[target_col].shift(-horizon)\n",
    "    # Align and drop NaNs caused by shifts\n",
    "    X = X.loc[~y.isna()]\n",
    "    y = y.loc[X.index]\n",
    "    return X, y\n",
    "# Build supervised data\n",
    "X, y = make_supervised(data, target_col='load', horizon=1)\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n",
    "# show a few rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027900f",
   "metadata": {},
   "source": [
    "## Section 3 — Time-based train / calibration / test split\n",
    "\n",
    "Random splits leak future information in time series. We'll split chronologically: TRAIN (first 60%), CALIBRATION (next 20%), TEST (last 20%). The calibration set will be used for conformal prediction intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d59e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chronological split\n",
    "n = len(X)\n",
    "i_train = int(n * 0.6)\n",
    "i_cal = int(n * 0.8)\n",
    "X_train = X.iloc[:i_train]\n",
    "y_train = y.iloc[:i_train]\n",
    "X_cal = X.iloc[i_train:i_cal]\n",
    "y_cal = y.iloc[i_train:i_cal]\n",
    "X_test = X.iloc[i_cal:]\n",
    "y_test = y.iloc[i_cal:]\n",
    "print('TRAIN range:', X_train.index.min(), 'to', X_train.index.max(), 'size', len(X_train))\n",
    "print('CAL range:', X_cal.index.min(), 'to', X_cal.index.max(), 'size', len(X_cal))\n",
    "print('TEST range:', X_test.index.min(), 'to', X_test.index.max(), 'size', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a28408",
   "metadata": {},
   "source": [
    "## Section 4 — Baseline model and ML model\n",
    "\n",
    "We compare a naive baseline (previous value) to a trained tree-based regressor. Evaluate using MAE and RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309a62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive baseline: predict using lag_1 (t-1). Align carefully.\n",
    "y_test_index = y_test.index\n",
    "# Baseline predictions: because target is t, naive is previous observed load at t-1 which is lag_1 feature\n",
    "yhat_naive = X_test['lag_1']\n",
    "# Fit a RandomForest as example ML model (students can replace with XGBoost/LGBM)\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "# TODO: fit the model on (X_train, y_train)\n",
    "model.fit(X_train, y_train)\n",
    "# Predict on test\n",
    "yhat = pd.Series(model.predict(X_test), index=X_test.index)\n",
    "# Evaluation\n",
    "def evaluate(y_true, y_pred, label='model'):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred)**0.5\n",
    "    print(f'{label} MAE: {mae:.3f}, RMSE: {rmse:.3f}')\n",
    "\n",
    "evaluate(y_test, yhat_naive, label='Naive baseline')\n",
    "evaluate(y_test, yhat, label='RandomForest')\n",
    "# Plot a selected TEST week: choose first week of test set\n",
    "try:\n",
    "    plot_start = X_test.index[0]\n",
    "    plot_end = plot_start + pd.Timedelta(days=7)\n",
    "    idx = (y_test.index >= plot_start) & (y_test.index < plot_end)\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(y_test.loc[idx].index, y_test.loc[idx].values, label='true')\n",
    "    plt.plot(yhat_naive.loc[idx].index, yhat_naive.loc[idx].values, label='naive')\n",
    "    plt.plot(yhat.loc[idx].index, yhat.loc[idx].values, label='model')\n",
    "    plt.legend()\n",
    "    plt.title('True vs Predictions (first test week)')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Could not plot test week:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3710ebcc",
   "metadata": {},
   "source": [
    "## Section 5 — Global and local XAI with SHAP\n",
    "\n",
    "We use SHAP to explain feature importance globally and locally. For tree models, TreeExplainer is efficient. For other models, KernelExplainer may be used but is slower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of X_test for SHAP analysis (to keep compute small)\n",
    "X_shap = X_test.sample(n=min(500, len(X_test)), random_state=RANDOM_SEED)\n",
    "# Use TreeExplainer for tree-based models\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "# Summary plot (beeswarm)\n",
    "shap.summary_plot(shap_values, X_shap, show=True)\n",
    "# Bar plot of mean absolute SHAP values\n",
    "shap.summary_plot(shap_values, X_shap, plot_type='bar', show=True)\n",
    "# Local explanation for a single interesting point (e.g., highest true load in test)\n",
    "idx_max = y_test.idxmax()\n",
    "print('Index with highest load in test:', idx_max)\n",
    "x_local = X_test.loc[[idx_max]]\n",
    "try:\n",
    "    sv_local = explainer.shap_values(x_local)\n",
    "    shap.plots.waterfall(sv_local[0] if isinstance(sv_local, list) else sv_local, max_display=12)\n",
    "except Exception as e:\n",
    "    print('Could not produce local SHAP waterfall:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d7d33",
   "metadata": {},
   "source": [
    "- TODO: Interpret SHAP results, are there any innecessary inputs? Try to simplify the model and check performance variations. Does SHAP results match domain intuition?\n",
    ">>> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e575ea8",
   "metadata": {},
   "source": [
    "## Section 6 — Bootstrap ensemble for predictive variability\n",
    "\n",
    "Train B bootstrap models (resampling training data with replacement) to approximate variability due to training data sampling. Use their distribution to produce prediction intervals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def bootstrap_models(X_train, y_train, base_model, B=10):\n",
    "    models = []\n",
    "    n = len(X_train)\n",
    "    for b in range(B):\n",
    "        idx = np.random.choice(np.arange(n), size=n, replace=True)\n",
    "        Xb = X_train.iloc[idx]\n",
    "        yb = y_train.iloc[idx]\n",
    "        m = deepcopy(base_model)\n",
    "        m.fit(Xb, yb)\n",
    "        models.append(m)\n",
    "    return models\n",
    "# Train B bootstrap models (may take time)\n",
    "B = 10\n",
    "base = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED)\n",
    "\n",
    "# TODO: If running time is a concern reduce B or model complexity\n",
    "models_b = bootstrap_models(X_train, y_train, base, B=B)\n",
    "# Compute bootstrap predictions for X_test (may be memory heavy; sample a week for plotting)\n",
    "def bootstrap_predict(models, X):\n",
    "    preds = np.vstack([m.predict(X) for m in models])  # shape (B, n)\n",
    "    return preds\n",
    "preds_b = bootstrap_predict(models_b, X_test)\n",
    "# Mean and 5th-95th percentile interval\n",
    "y_mean = preds_b.mean(axis=0)\n",
    "y_p05 = np.percentile(preds_b, 5, axis=0)\n",
    "y_p95 = np.percentile(preds_b, 95, axis=0)\n",
    "# Plot a test week with bootstrap interval\n",
    "try:\n",
    "    plot_start = X_test.index[0]\n",
    "    plot_end = plot_start + pd.Timedelta(days=7)\n",
    "    mask = (X_test.index >= plot_start) & (X_test.index < plot_end)\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(y_test.loc[mask].index, y_test.loc[mask].values, label='true')\n",
    "    plt.plot(X_test.loc[mask].index, y_mean[mask], label='bootstrap mean')\n",
    "    plt.fill_between(X_test.loc[mask].index, y_p05[mask], y_p95[mask], color='C0', alpha=0.3, label='5-95%')\n",
    "    plt.legend()\n",
    "    plt.title('Bootstrap ensemble forecast and interval (first test week)')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Could not plot bootstrap week:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba82b5",
   "metadata": {},
   "source": [
    "## Section 7 — Conformal prediction intervals (inductive residual-based)\n",
    "\n",
    "Inductive conformal uses residuals on a held-out calibration set to form a distribution of absolute errors; the quantile gives a symmetric interval around point predictions with finite-sample marginal coverage guarantees (under exchangeability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d24324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conformal quantile from calibration residuals ---\n",
    "\n",
    "# Train a fresh model on TRAIN if desired. We'll reuse `model` already trained, but you can retrain here.\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on calibration set and compute residuals\n",
    "yhat_cal = pd.Series(model.predict(X_cal), index=X_cal.index)\n",
    "residuals = (y_cal - yhat_cal).abs()\n",
    "\n",
    "def conformal_q(residuals, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Compute conformal quantile with finite-sample correction:\n",
    "    quantile at ceil((n+1)*(1-alpha))/n.\n",
    "    \"\"\"\n",
    "    n = len(residuals)\n",
    "    k = int(np.ceil((n + 1) * (1 - alpha))) - 1\n",
    "    q = np.sort(residuals)[k]\n",
    "    return q\n",
    "\n",
    "alpha = 0.1\n",
    "q_alpha = conformal_q(residuals.values, alpha=alpha)\n",
    "print(f'Conformal q (alpha={alpha}):', q_alpha)\n",
    "\n",
    "# --- Build intervals on TEST ---\n",
    "\n",
    "yhat_test = pd.Series(model.predict(X_test), index=X_test.index)\n",
    "lower = yhat_test - q_alpha\n",
    "upper = yhat_test + q_alpha\n",
    "\n",
    "# Evaluate empirical coverage and average width\n",
    "inside = ((y_test >= lower) & (y_test <= upper)).mean()\n",
    "avg_width = (upper - lower).mean()\n",
    "print(f'Coverage on TEST (alpha={alpha}): {inside:.3f}, avg width: {avg_width:.3f}')\n",
    "\n",
    "# Optionally loop over several alphas\n",
    "alphas = [0.05, 0.1, 0.2]\n",
    "rows = []\n",
    "for a in alphas:\n",
    "    q = conformal_q(residuals.values, alpha=a)\n",
    "    yhat_test_a = pd.Series(model.predict(X_test), index=X_test.index)\n",
    "    l = yhat_test_a - q\n",
    "    u = yhat_test_a + q\n",
    "    cov = ((y_test >= l) & (y_test <= u)).mean()\n",
    "    rows.append({'alpha': a, 'q': q, 'coverage': cov, 'avg_width': (u - l).mean()})\n",
    "\n",
    "conformal_summary = pd.DataFrame(rows)\n",
    "print(conformal_summary)\n",
    "\n",
    "# --- Plot conformal predictions along the time series (first 7 days of TEST) ---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define plot window: first 7 days of the test period\n",
    "plot_start = X_test.index[0]\n",
    "plot_end = plot_start + pd.Timedelta(days=7)\n",
    "\n",
    "mask = (y_test.index >= plot_start) & (y_test.index <= plot_end)\n",
    "\n",
    "y_test_plot = y_test[mask]\n",
    "yhat_plot = yhat_test[mask]\n",
    "lower_plot = lower[mask]\n",
    "upper_plot = upper[mask]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.plot(y_test_plot.index, y_test_plot, label='Actual')\n",
    "ax.plot(yhat_plot.index, yhat_plot, label='Prediction')\n",
    "ax.fill_between(yhat_plot.index, lower_plot, upper_plot, alpha=0.3, label='Conformal interval')\n",
    "\n",
    "ax.set_title(f'Conformal prediction interval on TEST (first 7 days, alpha={alpha})')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Load')\n",
    "ax.legend()\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e2a17",
   "metadata": {},
   "source": [
    "- TODO: Comment on the results, which you find more useful: bootstrap or conformal? Which would you use in case I ask you about uncertainty in forecasting models?\n",
    ">>> Your comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597d064",
   "metadata": {},
   "source": [
    "## Section 9 — Using intervals for anomaly flagging\n",
    "\n",
    "Flag times where true load lies outside conformal intervals — potentially anomalous events requiring operator attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d130090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using alpha chosen earlier\n",
    "yhat_test = pd.Series(model.predict(X_test), index=X_test.index)\n",
    "lower = yhat_test - q_alpha\n",
    "upper = yhat_test + q_alpha\n",
    "anomaly = ~( (y_test >= lower) & (y_test <= upper) )\n",
    "anomalies = pd.DataFrame({'y_true': y_test, 'yhat': yhat_test, 'lower': lower, 'upper': upper, 'anomaly': anomaly})\n",
    "print('Number of anomalies in TEST:', anomalies['anomaly'].sum())\n",
    "# Plot a test week with anomalies marked\n",
    "try:\n",
    "    mask = (X_test.index >= X_test.index[0]) & (X_test.index < X_test.index[0] + pd.Timedelta(days=7))\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(anomalies.loc[mask].index, anomalies.loc[mask, 'y_true'], label='true')\n",
    "    plt.plot(anomalies.loc[mask].index, anomalies.loc[mask, 'yhat'], label='yhat')\n",
    "    plt.fill_between(anomalies.loc[mask].index, anomalies.loc[mask, 'lower'], anomalies.loc[mask, 'upper'], color='C0', alpha=0.2)\n",
    "    # mark anomalies\n",
    "    an_idx = anomalies.loc[mask & anomalies['anomaly']].index\n",
    "    plt.scatter(an_idx, anomalies.loc[an_idx, 'y_true'], color='red', label='anomaly')\n",
    "    plt.legend()\n",
    "    plt.title('Test week with conformal band and anomalies')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print('Could not plot anomalies:', e)\n",
    "# Print a small table of anomalies (timestamp, y_true, yhat, lower, upper)\n",
    "anomalies[anomalies['anomaly']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e56680",
   "metadata": {},
   "source": [
    "- TODO: Are the anomalies true grid events or model errors? Justify your answer\n",
    ">>> Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4709ddd",
   "metadata": {},
   "source": [
    "## Section 10 — Visualizing\n",
    "\n",
    "- How would you present these results to non-technical stakeholders at a grid operator? Include visuals and short bullet recommendations. Add as many code and markdown cells as you find necessary.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Laboratory 03 - Timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
